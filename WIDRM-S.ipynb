{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e20924-d47a-45fc-921e-8e7ee9117c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os,sys,cv2,gc\n",
    "from torch.cuda.amp import autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import segmentation_models_pytorch as smp\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parallel import DataParallel\n",
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1c80f-ede6-4e0f-8434-9247066f7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    #Predata\n",
    "    predata = False\n",
    "    output_dir = 'EXP_MyDataSet'\n",
    "    target_height = 512\n",
    "    target_width = 512\n",
    "    wandb = False\n",
    "    seed = 42\n",
    "    project = 'Spine'\n",
    "    exp_name = 'exp01'\n",
    "    n_fold = 5\n",
    "    valid_fold = 4\n",
    "    chopping_percentile = 1e-3\n",
    "    in_chans = 1\n",
    "    train_batch_size = 4\n",
    "    valid_batch_size = 8\n",
    "    \n",
    "    train_aug_list = [\n",
    "        A.Affine(scale={\"x\":(0.8, 1.2), \"y\":(0.8, 1.2)}, translate_percent={\"x\":(0, 0.1), \"y\":(0, 0.1)}, rotate=(-30, 30), shear=(-20, 20), p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=0.5),\n",
    "        A.ShiftScaleRotate(scale_limit=0.2),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.OneOf([\n",
    "        #         A.RandomBrightnessContrast(), \n",
    "        #         A.RandomGamma(),\n",
    "        #   ],p=0.5,),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    train_aug = A.Compose(train_aug_list)\n",
    "    valid_aug_list = [\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    valid_aug = A.Compose(valid_aug_list)\n",
    "\n",
    "    nprocs=1 \n",
    "    fold_num=5 \n",
    "    num_classes=1\n",
    "    \n",
    "    accum_iter=1 \n",
    "    max_grad_norm=1e3\n",
    "    print_freq=100 \n",
    "    \n",
    "    test_fold_list=[0] \n",
    "    valid_fold_list=[1]\n",
    "    train_fold_list = [2,3,4]\n",
    "\n",
    "    epochs=25\n",
    "    \n",
    "    model_arch=\"efficientnet-b1\"\n",
    "    \n",
    "    optimizer=\"AdamW\" \n",
    "    \n",
    "    scheduler=\"CosineAnnealingLR\"\n",
    "    loss_fn=\"BCEWithLogitsLoss\"\n",
    "    scheduler_warmup= \"GradualWarmupSchedulerV3\"\n",
    "\n",
    "    warmup_epo = 10\n",
    "    warmup_factor = 10\n",
    "    T_max= epochs-warmup_epo-2 if scheduler_warmup==\"GradualWarmupSchedulerV2\" else \\\n",
    "           epochs-warmup_epo-1 if scheduler_warmup==\"GradualWarmupSchedulerV3\" else epochs-1\n",
    "    lr=1e-3 \n",
    "    min_lr=1e-6 #\n",
    "    weight_decay=1e-2\n",
    "    n_early_stopping=20\n",
    "    \n",
    "    model_name = 'seresnext26d_32x4d.bt_in1k'\n",
    "    model_path = 'Encoder_backbone/Encoder_backbone/Encoder/seresnext26d_32x4d_bt_in1k.bin' #0.8827\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e061980-7a00-4e70-b37e-1b39d2f89f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True \n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n",
    "\n",
    "# 日志记录函数\n",
    "def init_logger(log_file):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger(CFG.output_dir+f'/train_{CFG.exp_name}.log')\n",
    "loginfo = LOGGER.info\n",
    "cusprint = print\n",
    "\n",
    "def get_timediff(time1,time2):\n",
    "    minute_,second_ = divmod(time2-time1,60)\n",
    "    return f\"{int(minute_):02d}:{int(second_):02d}\"  \n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13f5fd-f07a-4400-aa2d-9a2203b7327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.predata:\n",
    "    os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "    def normalize_upsample(image,is_label):\n",
    "        # 缩放图像数据到0-255范围内以适应PNG格式\n",
    "        if not is_label:\n",
    "            image = image.astype(np.float32)\n",
    "            min_val = np.min(image)\n",
    "            max_val = np.max(image)\n",
    "            image = (image - min_val) / (max_val - min_val + 1e-9) * 255\n",
    "        image = cv2.resize(image, (CFG.target_width, CFG.target_height), interpolation=cv2.INTER_CUBIC)\n",
    "        return image.astype(np.uint8)\n",
    "        \n",
    "    # 用于存储所有信息的DataFrame\n",
    "    all_image_info = []\n",
    "    # 遍历定义的stage\n",
    "    for stage in CFG.stages:\n",
    "        #使用logger输出当前stage\n",
    "        logger.info(f\"当前阶段：{stage}\")\n",
    "        img_dir = os.path.join(CFG.top_level_dir, stage, 'image')\n",
    "        groundtruth_dir = os.path.join(CFG.top_level_dir, stage, 'groundtruth')\n",
    "        img_output_dir = os.path.join(CFG.output_dir, stage,'image')\n",
    "        groundtruth_output_dir = os.path.join(CFG.output_dir, stage,'groundtruth')\n",
    "        os.makedirs(img_output_dir, exist_ok=True)\n",
    "        os.makedirs(groundtruth_output_dir, exist_ok=True)\n",
    "        \n",
    "        # 遍历文件夹内的文件\n",
    "        for file_name in tqdm(os.listdir(img_dir)):\n",
    "            if file_name.endswith('.nii.gz'):\n",
    "                # 读取image和groundtruth文件\n",
    "                img_path = os.path.join(img_dir, file_name)\n",
    "                groundtruth_path = os.path.join(groundtruth_dir, f'mask_{file_name.lower()}')\n",
    "                try:\n",
    "                    img = nib.load(img_path).get_fdata()\n",
    "                    label = nib.load(groundtruth_path).get_fdata()\n",
    "                    height, width, depth = img.shape\n",
    "                    \n",
    "                    # 检查shape是否相同\n",
    "                    if img.shape != label.shape:\n",
    "                        raise ValueError(\"Image and label shape do not match.\")\n",
    "                except FileNotFoundError as e:\n",
    "                    logger.warning(f\"文件未找到: {e}\")\n",
    "                    continue\n",
    "                except ValueError as e:\n",
    "                    logger.warning(f\"Shape不匹配警告: {e} - 跳过文件 {file_name}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"处理文件 {file_name} 时发生未知错误: {e}\")\n",
    "                    continue\n",
    "                base_name = file_name.split('.')[0]\n",
    "                # 遍历所有切片并保存为PNG文件\n",
    "                for i in range(img.shape[2]):\n",
    "                    image_2d = img[:, :, i]\n",
    "                    label_2d = label[:, :, i]\n",
    "                    img_filename = f'{base_name}_{i}.png'\n",
    "                    gt_filename = f'{base_name}_{i}_label.png'\n",
    "                    img_png_path = os.path.join(img_output_dir, img_filename)\n",
    "                    gt_png_path = os.path.join(groundtruth_output_dir, gt_filename)\n",
    "                    \n",
    "                    # 保存图片和标签的PNG文件\n",
    "                    imageio.imwrite(img_png_path,normalize_upsample(image_2d,is_label=False))\n",
    "                    imageio.imwrite(gt_png_path,normalize_upsample(label_2d,is_label=True))\n",
    "                    # 将信息添加到列表中\n",
    "                    all_image_info.append({\n",
    "                        'Image': img_filename,\n",
    "                        'Case':base_name,\n",
    "                        'ImagePath': img_png_path,\n",
    "                        'GroundTruthPath': gt_png_path,\n",
    "                        'height':height,\n",
    "                        'width':width,\n",
    "                        'Stage': stage\n",
    "                    })\n",
    "    \n",
    "    # 使用列表创建DataFrame\n",
    "    df = pd.DataFrame(all_image_info)\n",
    "    # 保存DataFrame为CSV文件\n",
    "    df.to_csv('image_groundtruth_data.csv', index=False)\n",
    "    logger.info(f\"处理完成，DataFrame已保存为 image_groundtruth_data.csv\")\n",
    "else:\n",
    "    df = pd.read_csv('image_groundtruth_data_my.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243739fb-173f-48bf-a08e-f7487153d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692a61f-55c5-4ea1-9dbc-124178735435",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.wandb:\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.login()\n",
    "        run = wandb.init(project=CFG.project, \n",
    "                 name=CFG.exp_name,\n",
    "                ) \n",
    "    except:\n",
    "        logger.info(f\"Check your WANDB account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5d062-25b8-43e3-a0fd-00f521e6a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=CFG.n_fold)\n",
    "df[\"fold\"] = -1\n",
    "for fold_id, (_, val_idx) in enumerate(\n",
    "    gkf.split(df, y=df[\"GroundTruthPath\"], groups=df[\"Case\"])\n",
    "):\n",
    "    df.loc[val_idx, \"fold\"] = fold_id\n",
    "df.fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ec241-e076-4803-b454-39d8ed861049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_loader2D(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df = df.reset_index()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        row = self.df.iloc[index]\n",
    "        img_path = row.ImagePath\n",
    "        label_path = row.GroundTruthPath\n",
    "        img = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "        label = cv2.imread(label_path,cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        img=torch.from_numpy(img)\n",
    "        label=torch.from_numpy(label)\n",
    "        bone_list=[]\n",
    "        for i in row.Gt_all_path[1:-1].split(', '):\n",
    "            i=i[1:-1]\n",
    "            bone = cv2.imread(i,cv2.IMREAD_GRAYSCALE)\n",
    "            bone = torch.from_numpy(bone)\n",
    "            bone_list.append(bone)\n",
    "        bone_list = torch.stack(bone_list)\n",
    "        return img,label,bone_list\n",
    "    \n",
    "def load_data(df):\n",
    "    data_loader=Data_loader2D(df)\n",
    "    data_loader=DataLoader(data_loader, batch_size=16, num_workers=0)\n",
    "    img=[]\n",
    "    label=[]\n",
    "    bone=[]\n",
    "    for x,y,z in tqdm(data_loader):\n",
    "        img.append(x)\n",
    "        label.append(y)\n",
    "        bone.append(z)\n",
    "    img_c=torch.cat(img,dim=0)\n",
    "    label_c=torch.cat(label,dim=0)\n",
    "    bone_c=torch.cat(bone,dim=0)\n",
    "    del img,label,bone\n",
    "    return img_c,label_c,bone_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da9c2b-a71c-4342-9fa6-4a4d21c226a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img,train_label,train_bone=load_data(df)\n",
    "print(train_img.shape)\n",
    "print(train_label.shape)\n",
    "print(train_bone.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc08be5-c297-495f-b3fe-560d4d87076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spine_Dataset(Dataset):\n",
    "    def __init__(self,x:list,y:list,z:list,arg=False):\n",
    "        super(Dataset,self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y #list[(C,H,W),...]\n",
    "        self.z = z\n",
    "        self.image_size=CFG.target_height\n",
    "        self.in_chans=CFG.in_chans\n",
    "        self.arg=arg\n",
    "        if arg:\n",
    "            self.transform=CFG.train_aug\n",
    "        else: \n",
    "            self.transform=CFG.valid_aug\n",
    "            \n",
    "    def __len__(self) -> int:\n",
    "        return sum([y.shape[0]-self.in_chans for y in self.y])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        i=0\n",
    "        for x in self.x:\n",
    "            if index>x.shape[0]-self.in_chans:\n",
    "                index-=x.shape[0]-self.in_chans\n",
    "                i+=1\n",
    "            else:\n",
    "                break\n",
    "        x=self.x[i]\n",
    "        y=self.y[i]\n",
    "        z=self.z[i]\n",
    "        \n",
    "        x=x[index:index+self.in_chans,:,:]\n",
    "        y=y[index+self.in_chans//2,:,:]\n",
    "        z=z[index+self.in_chans//2,:,:,:]\n",
    "        y = y.unsqueeze(0) \n",
    "        y = torch.cat((y, z), dim=0) # 使用concat进行拼接操作\n",
    "\n",
    "        if self.in_chans == 1:\n",
    "             x = x.repeat(3, 1, 1)\n",
    "        data = self.transform(image=x.numpy().transpose(1, 2, 0), mask=y.numpy().transpose(1, 2, 0))\n",
    "        x = data['image']\n",
    "        y = data['mask']\n",
    "        # 创建二值标签mask：所有不等于0的标签变为1\n",
    "        mask = torch.where(y != 0, torch.tensor(1.0, dtype=torch.float32), torch.tensor(0.0, dtype=torch.float32))\n",
    "        binary_mask = mask[0:1]\n",
    "        multilabel_mask = mask[1:]\n",
    "        # 返回最终的输入数据x，二值标签binary_mask，以及多标签multilabel_mask\n",
    "        return x, binary_mask, multilabel_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faceb6ea-9b31-4f25-b7a7-19e0418634ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Spine_Dataset([train_img],[train_label],[train_bone],arg=True)\n",
    "train_dataset = DataLoader(train_dataset, batch_size=CFG.train_batch_size ,num_workers=0, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a229f-e276-4bd2-b9da-9f4594ef6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取DataLoader的迭代器\n",
    "data_iterator = iter(train_dataset)\n",
    "# 从迭代器中获取第一个批次\n",
    "first_batch = next(data_iterator)\n",
    "print('img shape:',first_batch[0].shape,'binary_label_shape:',first_batch[1].shape,'multi_label_shape:',first_batch[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c3040-70c5-4178-9301-2c333de4c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "def build_model():\n",
    "    model = smp.DeepLabV3Plus(\n",
    "        encoder_name=CFG.model_arch,    # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n",
    "        )\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(path):\n",
    "    model = build_model()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae58522-7c5f-4c51-8494-66f8f7e13186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SELayer(nn.Module): \n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel//reduction,bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel//reduction,channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.size()\n",
    "        y = self.avgpool(x).view(b,c)\n",
    "        y = self.fc(y).view(b,c,1,1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bs, c, h, w = 10, 16, 64, 64\n",
    "    in_tensor = torch.ones(bs, c, h, w)\n",
    "\n",
    "    cs_se = SELayer(c)\n",
    "    print(\"in shape:\",in_tensor.shape)\n",
    "    out_tensor = cs_se(in_tensor)\n",
    "    print(\"out shape:\", out_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ca640-a45e-4d34-ad55-ea957360d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class sSE(nn.Module): \n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.Conv1x1 = nn.Conv2d(in_channels, 1, kernel_size=1, bias=False)\n",
    "        self.norm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, U):\n",
    "        q = self.Conv1x1(U) \n",
    "        q = self.norm(q)\n",
    "        return U * q  \n",
    "\n",
    "class cSE(nn.Module): \n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.Conv_Squeeze = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1, bias=False)\n",
    "        self.Conv_Excitation = nn.Conv2d(in_channels//2, in_channels, kernel_size=1, bias=False)\n",
    "        self.norm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, U):\n",
    "        z = self.avgpool(U)# shape: [bs, c, h, w] to [bs, c, 1, 1]\n",
    "        z = self.Conv_Squeeze(z) # shape: [bs, c/2]\n",
    "        z = self.Conv_Excitation(z) # shape: [bs, c]\n",
    "        z = self.norm(z)\n",
    "        return U * z.expand_as(U)\n",
    "\n",
    "class csSE(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.cSE = cSE(in_channels)\n",
    "        self.sSE = sSE(in_channels)\n",
    "\n",
    "    def forward(self, U):\n",
    "        U_sse = self.sSE(U)\n",
    "        U_cse = self.cSE(U)\n",
    "        return U_cse+U_sse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bs, c, h, w = 10, 3, 64, 64\n",
    "    in_tensor = torch.ones(bs, c, h, w)\n",
    "    cs_se = csSE(c)\n",
    "    print(\"in shape:\",in_tensor.shape)\n",
    "    out_tensor = cs_se(in_tensor)\n",
    "    print(\"out shape:\", out_tensor.shape)\n",
    "\n",
    "    s_se = sSE(c)\n",
    "    print(\"in shape:\",in_tensor.shape)\n",
    "    out_tensor = s_se(in_tensor)\n",
    "    print(\"out shape:\", out_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1a2fa-58a1-4a98-9f73-5a0032f1def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.sharedMLP = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False), nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('X shape：',x.shape)\n",
    "        mid_col_idx_start = x.shape[3] // 4 \n",
    "        mid_col_idx_end = x.shape[3] // 4 *3\n",
    "        \n",
    "        # 仅抽取中间列，中间列形状为 [batch_size, channels, height, 1]\n",
    "        mid_col = x[:, :, :, mid_col_idx_start:mid_col_idx_end]\n",
    "        print('Mid col shape:', mid_col.shape)\n",
    "        \n",
    "        print('Avg shape：',self.avg_pool(mid_col).shape)\n",
    "        avgout = self.sharedMLP(self.avg_pool(x))\n",
    "        maxout = self.sharedMLP(self.max_pool(x))\n",
    "        return self.sigmoid(avgout + maxout)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), \"kernel size must be 3 or 7\"\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgout = torch.mean(x, dim=1, keepdim=True)\n",
    "        maxout, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avgout, maxout], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.ca = ChannelAttention(planes)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.ca(out) * out  # 广播机制\n",
    "        out = self.sa(out) * out  # 广播机制\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x = torch.ones(3, 16, 32, 32)\n",
    "\n",
    "    model = BasicBlock(16, 16, stride=1)\n",
    "\n",
    "    print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9473c-a813-4bd6-bf56-fefb5691d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pool_variance(Z_prev, f_h=2, f_w=2, padding=0, stride_h=2, stride_w=3):\n",
    "    (b, n_C_prev, n_H_prev, n_W_prev) = Z_prev.shape\n",
    "\n",
    "    n_H = 1 + int((n_H_prev + 2 * padding - f_h) / stride_h)\n",
    "    n_W = 1 + int((n_W_prev + 2 * padding - f_w) / stride_w)\n",
    "\n",
    "    Z_prev_unfold = torch.nn.functional.unfold(Z_prev, (f_h, f_w), stride=(stride_h, stride_w)) # (b, n_C_prev * f_h * f_w, L)\n",
    "    Z_prev_unfold = Z_prev_unfold.transpose(1, 2) # (b, L, n_C_prev * f_h * f_w)\n",
    "    Z_prev_unfold = Z_prev_unfold.view(b, n_H * n_W, n_C_prev, -1) # (b, L, n_C_prev, f_h * f_w)\n",
    "    \n",
    "    mean_squared = torch.mean(Z_prev_unfold ** 2, dim=3, keepdim=False)\n",
    "    \n",
    "    mean = torch.mean(Z_prev_unfold, dim=3, keepdim=False)\n",
    "    \n",
    "    variance = mean_squared - mean ** 2\n",
    "    \n",
    "    variance = variance.transpose(1, 2)\n",
    "    \n",
    "    Z_var = torch.nn.functional.fold(variance, (n_H, n_W), (1, 1))\n",
    "    \n",
    "    assert(Z_var.size() == (b, n_C_prev, n_H, n_W))\n",
    "    return Z_var\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_input = torch.randn(1, 64, 32, 16)  \n",
    "    result = pool_variance(sample_input, f_h=8, f_w=16, padding=0, stride_h=8, stride_w=16)\n",
    "    print(result.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2407b04-80a1-43d2-932a-564fd0350945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.sharedMLP = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_planes// ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_planes),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.height_reduce = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes, (8, 2), bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mid_col_idx_start = x.shape[3]//4\n",
    "        mid_col_idx_end = x.shape[3]//4 *3\n",
    "\n",
    "        mid_col = x[:, :, :, mid_col_idx_start:mid_col_idx_end]\n",
    "\n",
    "        pool_height = mid_col.shape[2] // 8\n",
    "        pool_width =  mid_col_idx_end - mid_col_idx_start\n",
    "        stride = (pool_height, pool_width)  # stride matches the pool size for non-overlapping pooling\n",
    "\n",
    "        avg_pool = nn.AvgPool2d((pool_height, pool_width), stride=stride)\n",
    "        var_pooled = pool_variance(mid_col,f_h=pool_height, f_w=pool_width, stride_h=pool_height, stride_w=pool_width)\n",
    "\n",
    "        avg_pooled = avg_pool(mid_col)\n",
    "   \n",
    "        avgout = self.sharedMLP(avg_pooled)\n",
    "        varout = self.sharedMLP(var_pooled)\n",
    "        concat_pooled = torch.cat((avgout,varout), dim=3)\n",
    "\n",
    "        mlp_output = self.height_reduce(concat_pooled)\n",
    "\n",
    "        return self.sigmoid(mlp_output)\n",
    "\n",
    "# 测试模块\n",
    "if __name__ == \"__main__\":\n",
    "    sample_input = torch.randn(4, 64, 32, 32) \n",
    "    channel_attention = ChannelAttention(in_planes=64)\n",
    "    output = channel_attention(sample_input)\n",
    "    print(output.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ee502-9081-4c22-b9f4-b9727441c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, planes, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), \"kernel size must be 3 or 7\"\n",
    "        self.kernel_size = kernel_size\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.padding = padding\n",
    "        self.conv = nn.Conv2d(3, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.p2pconv = nn.Conv2d(planes,1,kernel_size,padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        convout = self.p2pconv(x)\n",
    "        avgout = torch.mean(x, dim=1, keepdim=True)\n",
    "        maxout, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        varout = torch.var(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avgout, varout,convout], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.ca = ChannelAttention(planes)\n",
    "        self.sa = SpatialAttention(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ca(x) * x  # 广播机制\n",
    "        out = self.sa(out) * out  # 广播机制\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x = torch.ones(3, 16, 32, 32)\n",
    "\n",
    "    model = BasicBlock(16, 16, stride=1)\n",
    "\n",
    "    print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0365f-4165-44c2-a5e1-139001861976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "class CoordAtt(nn.Module):  \n",
    "    def __init__(self, inp, oup, reduction=32):\n",
    "        super(CoordAtt, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mip = max(8, inp // reduction)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)\n",
    "        self.act = h_swish()\n",
    "        \n",
    "        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        n,c,h,w = x.size()\n",
    "        x_h = self.pool_h(x)\n",
    "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
    "\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act(y) \n",
    "        \n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        a_h = self.conv_h(x_h).sigmoid()\n",
    "        a_w = self.conv_w(x_w).sigmoid()\n",
    "\n",
    "        out = identity * a_w * a_h\n",
    "\n",
    "        return out\n",
    "if __name__ == \"__main__\":\n",
    "    bs, c, h, w = 10, 16, 64, 64\n",
    "    in_tensor = torch.ones(bs, c, h, w)\n",
    "\n",
    "    cs_se = CoordAtt(c,c)\n",
    "    print(\"in shape:\",in_tensor.shape)\n",
    "    out_tensor = cs_se(in_tensor)\n",
    "    print(\"out shape:\", out_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5f6c6-2758-4c38-a46d-ab07101348d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MS_CAM(nn.Module): \n",
    "    def __init__(self, channels=64, r=8):\n",
    "        super(MS_CAM, self).__init__()\n",
    "        inter_channels = int(channels // r)\n",
    "\n",
    "        self.local_att = nn.Sequential(\n",
    "            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(inter_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(channels),\n",
    "        )\n",
    "\n",
    "        self.global_att = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, inter_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(inter_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inter_channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(channels),\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        xl = self.local_att(x)\n",
    "        xg = self.global_att(x)\n",
    "        xlg = xl + xg\n",
    "        wei = self.sigmoid(xlg)\n",
    "        return x * wei\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bs, c, h, w = 10, 16, 64, 64\n",
    "    in_tensor = torch.ones(bs, c, h, w)\n",
    "\n",
    "    cs_se = MS_CAM(c)\n",
    "    print(\"in shape:\",in_tensor.shape)\n",
    "    out_tensor = cs_se(in_tensor)\n",
    "    print(\"out shape:\", out_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b943d2-c02e-4137-b301-3304df3e1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, input_channels:list, output_channels:list):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=1),\n",
    "             nn.ReLU(inplace=True), nn.BatchNorm2d(out_ch*2),\n",
    "             nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n",
    "            for in_ch, out_ch in zip(input_channels, output_channels)])\n",
    "    def forward(self, xs:list, last_layer):\n",
    "        hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i+1),mode='bilinear') \n",
    "               for i,(c,x) in enumerate(zip(self.convs, xs))]\n",
    "        hcs.append(last_layer)\n",
    "        return torch.cat(hcs, dim=1)\n",
    "\n",
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n",
    "                 self_attention:bool=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(x_in_c)\n",
    "        self.bn2 = nn.BatchNorm2d(nf)             \n",
    "        ni = up_in_c//2 + x_in_c\n",
    "        nf = nf if nf is not None else max(up_in_c//2,32)\n",
    "        self.conv1 = ConvLayer(ni, nf, norm_type=None, **kwargs)\n",
    "        self.conv2 = ConvLayer(nf, nf, norm_type=None,\n",
    "            xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.nf = nf\n",
    "    def forward(self, up_in, left_in):\n",
    "        s = left_in\n",
    "        #c_se = csSE(s.shape[1]).cuda()  \n",
    "        #c_se = SELayer(s.shape[1]).cuda() \n",
    "        #c_se = BasicBlock(s.shape[1], s.shape[1], stride=1).cuda() \n",
    "        #c_se = CoordAtt(s.shape[1],s.shape[1]).cuda() \n",
    "        #s = c_se(s)\n",
    "        up_out = self.shuf(up_in)\n",
    "        cat_x = self.gelu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "        return self.conv2(self.gelu(self.bn2(self.conv1(cat_x))))\n",
    "        \n",
    "class UnetBlockWithAtt(nn.Module):\n",
    "    def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n",
    "                 self_attention:bool=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(x_in_c)\n",
    "        self.bn2 = nn.BatchNorm2d(nf)             \n",
    "        ni = up_in_c//2 + x_in_c\n",
    "        nf = nf if nf is not None else max(up_in_c//2,32)\n",
    "        self.conv1 = ConvLayer(ni, nf, norm_type=None, **kwargs)\n",
    "        self.conv2 = ConvLayer(nf, nf, norm_type=None,\n",
    "            xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.nf = nf\n",
    "    def forward(self, up_in, left_in):\n",
    "        s = left_in\n",
    "        #c_se = csSE(s.shape[1]).cuda()  #0.8832\n",
    "        #c_se = SELayer(s.shape[1]).cuda() #0.8789\n",
    "        c_se = BasicBlock(s.shape[1], s.shape[1], stride=1).cuda() #0.8833 CBAM\n",
    "        #c_se = CoordAtt(s.shape[1],s.shape[1]).cuda()  #0.8845\n",
    "        s = c_se(s)\n",
    "        up_out = self.shuf(up_in)\n",
    "        cat_x = self.gelu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "        return self.conv2(self.gelu(self.bn2(self.conv1(cat_x))))\n",
    "        \n",
    "class _ASPPModule(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size, padding, dilation, groups=1):\n",
    "        super().__init__()\n",
    "        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n",
    "                stride=1, padding=padding, dilation=dilation, bias=False, groups=groups)\n",
    "        self.bn = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.atrous_conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, inplanes=512, mid_c=256, dilations=[6, 12, 18, 24], out_c=None):\n",
    "        super().__init__()\n",
    "        self.aspps = [_ASPPModule(inplanes, mid_c, 1, padding=0, dilation=1)] + \\\n",
    "            [_ASPPModule(inplanes, mid_c, 3, padding=d, dilation=d,groups=4) for d in dilations]\n",
    "        self.aspps = nn.ModuleList(self.aspps)\n",
    "        self.global_pool = nn.Sequential(nn.AdaptiveMaxPool2d((1, 1)),\n",
    "                        nn.Conv2d(inplanes, mid_c, 1, stride=1, bias=False),\n",
    "                        nn.BatchNorm2d(mid_c), nn.ReLU(inplace=True))\n",
    "        out_c = out_c if out_c is not None else mid_c\n",
    "        self.out_conv = nn.Sequential(nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False),\n",
    "                                    nn.BatchNorm2d(out_c), nn.ReLU(inplace=True))\n",
    "        self.conv1 = nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.global_pool(x)\n",
    "        xs = [aspp(x) for aspp in self.aspps]\n",
    "        x0 = F.interpolate(x0, size=xs[0].size()[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x0] + xs, dim=1)\n",
    "        return self.out_conv(x)\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34dfaa5-e7e7-457e-8ee1-0171eb07c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WIDRMS(nn.Module):\n",
    "    def __init__(self, stride=1, **kwargs):\n",
    "        super().__init__()\n",
    "        #encoder\n",
    "        m = timm.create_model(CFG.model_name, pretrained=False)\n",
    "        #weights = torch.load(CFG.model_path, map_location=torch.device('cpu'))\n",
    "        # 应用这些权重到模型上\n",
    "        #m.load_state_dict(weights)\n",
    "        self.enc0 = nn.Sequential(m.conv1, m.bn1, nn.ReLU(inplace=True))\n",
    "        self.enc1 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "                            m.layer1) #256\n",
    "        self.enc2 = m.layer2 #512\n",
    "        self.enc3 = m.layer3 #1024\n",
    "        self.enc4 = m.layer4 #2048\n",
    "        #aspp with customized dilatations\n",
    "        self.aspp = ASPP(2048,256,out_c=512,dilations=[stride*1,stride*2,stride*3,stride*4])\n",
    "        self.drop_aspp = nn.Dropout2d(0.0)\n",
    "        #decoder\n",
    "        self.dec4 = UnetBlock(512,1024,256)\n",
    "        self.dec3 = UnetBlock(256,512,128)\n",
    "        self.dec2 = UnetBlockWithAtt(128,256,64)\n",
    "        self.dec1 = UnetBlockWithAtt(64,64,32)\n",
    "        self.fpn = FPN([512,256,128,64],[16]*4)\n",
    "        self.drop = nn.Dropout2d(0.0)\n",
    "        self.final_conv = ConvLayer(32+16*4, CFG.num_classes, ks=1, norm_type=None, act_cls=None)\n",
    "        #self.final_conv = ConvLayer(32, CFG.num_classes, ks=1, norm_type=None, act_cls=None)\n",
    "        self.num_classes = CFG.num_classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc0 = self.enc0(x)\n",
    "        enc1 = self.enc1(enc0)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        enc5 = self.aspp(enc4)\n",
    "        dec3 = self.dec4(self.drop_aspp(enc5),enc3)\n",
    "        #dec3 = self.dec4(enc4,enc3)\n",
    "        dec2 = self.dec3(dec3,enc2)\n",
    "        dec1 = self.dec2(dec2,enc1)\n",
    "        dec0 = self.dec1(dec1,enc0)\n",
    "        dec0 = F.interpolate(dec0,scale_factor=2,mode='bilinear')\n",
    "        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n",
    "        x = self.final_conv(self.drop(x))\n",
    "        #x = self.final_conv(self.drop(dec0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2426bfe-493c-4d4b-ba91-be7f1ad88366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from fastai.vision.all import PixelShuffle_ICNR\n",
    "from fastai.vision.all import ConvLayer,SelfAttention\n",
    "from fastai.vision.all import AdaptiveConcatPool2d, Flatten\n",
    "\n",
    "import torch\n",
    "model = WIDRMS().cuda()\n",
    "# state_dict = torch.load('Pretrained_Weight/seresnext26d_32x4d.bt_in1k_exp01_fold0_epoch24.pth')\n",
    "# if 'model' in state_dict.keys():\n",
    "#     state_dict = state_dict['model']\n",
    "# model.load_state_dict(state_dict)\n",
    "input_tensor = torch.randn(2, 3, 512, 512).cuda()\n",
    "output = model(input_tensor)\n",
    "print('Input shape:', input_tensor.shape)\n",
    "print('Output shape:', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30070862-0855-4793-89cf-1e738bbd3cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedNeXtBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_channels:int, \n",
    "                out_channels:int, \n",
    "                exp_r:int=4, \n",
    "                kernel_size:int=7, \n",
    "                do_res:int=True,\n",
    "                norm_type:str = 'group',\n",
    "                n_groups:int or None = None,\n",
    "                dim = '2d',\n",
    "                grn = False\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.do_res = do_res\n",
    "        assert dim in ['2d', '3d']\n",
    "        self.dim = dim\n",
    "        if self.dim == '2d':\n",
    "            conv = nn.Conv2d\n",
    "        elif self.dim == '3d':\n",
    "            conv = nn.Conv3d\n",
    "        # First convolution layer with DepthWise Convolutions\n",
    "        self.conv1 = conv(\n",
    "            in_channels = in_channels,\n",
    "            out_channels = in_channels,\n",
    "            kernel_size = kernel_size,\n",
    "            stride = 1,\n",
    "            padding = kernel_size//2,\n",
    "            groups = in_channels if n_groups is None else n_groups,\n",
    "        )\n",
    "        print()\n",
    "        # Normalization Layer. GroupNorm is used by default.\n",
    "        if norm_type=='group':\n",
    "            self.norm = nn.GroupNorm(\n",
    "                num_groups=in_channels, \n",
    "                num_channels=in_channels\n",
    "                )\n",
    "        elif norm_type=='layer':\n",
    "            self.norm = LayerNorm(\n",
    "                normalized_shape=in_channels, \n",
    "                data_format='channels_first'\n",
    "                )\n",
    "\n",
    "        # Second convolution (Expansion) layer with Conv3D 1x1x1\n",
    "        self.conv2 = conv(\n",
    "            in_channels = in_channels,\n",
    "            out_channels = exp_r*in_channels,\n",
    "            kernel_size = 1,\n",
    "            stride = 1,\n",
    "            padding = 0\n",
    "        )\n",
    "        \n",
    "        # GeLU activations\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        # Third convolution (Compression) layer with Conv3D 1x1x1\n",
    "        self.conv3 = conv(\n",
    "            in_channels = exp_r*in_channels,\n",
    "            out_channels = out_channels,\n",
    "            kernel_size = 1,\n",
    "            stride = 1,\n",
    "            padding = 0\n",
    "        )\n",
    "\n",
    "        self.grn = grn\n",
    "        if grn:\n",
    "            if dim == '3d':\n",
    "                self.grn_beta = nn.Parameter(torch.zeros(1,exp_r*in_channels,1,1,1), requires_grad=True)\n",
    "                self.grn_gamma = nn.Parameter(torch.zeros(1,exp_r*in_channels,1,1,1), requires_grad=True)\n",
    "            elif dim == '2d':\n",
    "                self.grn_beta = nn.Parameter(torch.zeros(1,exp_r*in_channels,1,1), requires_grad=True)\n",
    "                self.grn_gamma = nn.Parameter(torch.zeros(1,exp_r*in_channels,1,1), requires_grad=True)\n",
    "\n",
    " \n",
    "    def forward(self, x, dummy_tensor=None):\n",
    "        x1 = x\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.act(self.conv2(self.norm(x1)))\n",
    "        if self.grn:\n",
    "            # gamma, beta: learnable affine transform parameters\n",
    "            # X: input of shape (N,C,H,W,D)\n",
    "            if self.dim == '3d':\n",
    "                gx = torch.norm(x1, p=2, dim=(-3, -2, -1), keepdim=True)\n",
    "            elif self.dim == '2d':\n",
    "                gx = torch.norm(x1, p=2, dim=(-2, -1), keepdim=True)\n",
    "            nx = gx / (gx.mean(dim=1, keepdim=True)+1e-6)\n",
    "            x1 = self.grn_gamma * (x1 * nx) + self.grn_beta + x1\n",
    "        x1 = self.conv3(x1)\n",
    "        if self.do_res:\n",
    "            x1 = x + x1  \n",
    "        return x1\n",
    "        \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class MedNeXtDownBlock(MedNeXtBlock):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, exp_r=4, kernel_size=7, \n",
    "                do_res=False, norm_type = 'group', dim='2d', grn=False):\n",
    "\n",
    "        super().__init__(in_channels, out_channels, exp_r, kernel_size, \n",
    "                        do_res = False, norm_type = norm_type, dim=dim,\n",
    "                        grn=grn)\n",
    "\n",
    "        if dim == '2d':\n",
    "            conv = nn.Conv2d\n",
    "        elif dim == '3d':\n",
    "            conv = nn.Conv3d\n",
    "        self.resample_do_res = do_res\n",
    "        if do_res:\n",
    "            self.res_conv = conv(\n",
    "                in_channels = in_channels,\n",
    "                out_channels = out_channels,\n",
    "                kernel_size = 1,\n",
    "                stride = 2\n",
    "            )\n",
    "\n",
    "        self.conv1 = conv(\n",
    "            in_channels = in_channels,\n",
    "            out_channels = in_channels,\n",
    "            kernel_size = kernel_size,\n",
    "            stride = 2,\n",
    "            padding = kernel_size//2,\n",
    "            groups = in_channels,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, dummy_tensor=None):\n",
    "        \n",
    "        x1 = super().forward(x)\n",
    "        \n",
    "        if self.resample_do_res:\n",
    "            res = self.res_conv(x)\n",
    "            x1 = x1 + res\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "class MedNeXtUpBlock(MedNeXtBlock):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, exp_r=4, kernel_size=7, \n",
    "                do_res=False, norm_type = 'group', dim='2d', grn = False):\n",
    "        super().__init__(in_channels, out_channels, exp_r, kernel_size,\n",
    "                         do_res=False, norm_type = norm_type, dim=dim,\n",
    "                         grn=grn)\n",
    "\n",
    "        self.resample_do_res = do_res\n",
    "        \n",
    "        self.dim = dim\n",
    "        if dim == '2d':\n",
    "            conv = nn.ConvTranspose2d\n",
    "        elif dim == '3d':\n",
    "            conv = nn.ConvTranspose3d\n",
    "        if do_res:            \n",
    "            self.res_conv = conv(\n",
    "                in_channels = in_channels,\n",
    "                out_channels = out_channels,\n",
    "                kernel_size = 1,\n",
    "                stride = 2\n",
    "                )\n",
    "\n",
    "        self.conv1 = conv(\n",
    "            in_channels = in_channels,\n",
    "            out_channels = in_channels,\n",
    "            kernel_size = kernel_size,\n",
    "            stride = 2,\n",
    "            padding = kernel_size//2,\n",
    "            groups = in_channels,\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, dummy_tensor=None):\n",
    "        \n",
    "        x1 = super().forward(x)\n",
    "        # Asymmetry but necessary to match shape\n",
    "        \n",
    "        if self.dim == '2d':\n",
    "            x1 = torch.nn.functional.pad(x1, (1,0,1,0))\n",
    "        elif self.dim == '3d':\n",
    "            x1 = torch.nn.functional.pad(x1, (1,0,1,0,1,0))\n",
    "        \n",
    "        if self.resample_do_res:\n",
    "            res = self.res_conv(x)\n",
    "            if self.dim == '2d':\n",
    "                res = torch.nn.functional.pad(res, (1,0,1,0))\n",
    "            elif self.dim == '3d':\n",
    "                res = torch.nn.functional.pad(res, (1,0,1,0,1,0))\n",
    "            x1 = x1 + res\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "class OutBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, n_classes, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        if dim == '2d':\n",
    "            conv = nn.ConvTranspose2d\n",
    "        elif dim == '3d':\n",
    "            conv = nn.ConvTranspose3d\n",
    "        self.conv_out = conv(in_channels, n_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x, dummy_tensor=None): \n",
    "        return self.conv_out(x)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-5, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))        # beta\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))         # gamma\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x, dummy_tensor=False):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None, None] * x + self.bias[:, None, None, None]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6b658-9664-491e-ab5d-2426cafbb97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedNeXt(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        in_channels: int, \n",
    "        n_channels: int,\n",
    "        n_classes: int, \n",
    "        exp_r: int = 4,                            # Expansion ratio as in Swin Transformers\n",
    "        kernel_size: int = 7,                      # Ofcourse can test kernel_size\n",
    "        enc_kernel_size: int = None,\n",
    "        dec_kernel_size: int = None,\n",
    "        deep_supervision: bool = False,             # Can be used to test deep supervision\n",
    "        do_res: bool = False,                       # Can be used to individually test residual connection\n",
    "        do_res_up_down: bool = False,             # Additional 'res' connection on up and down convs\n",
    "        checkpoint_style: bool = None,            # Either inside block or outside block\n",
    "        block_counts: list = [2,2,2,2,2,2,2,2,2], # Can be used to test staging ratio: \n",
    "                                            # [3,3,9,3] in Swin as opposed to [2,2,2,2,2] in nnUNet\n",
    "        norm_type = 'group',\n",
    "        dim = '2d',                                # 2d or 3d\n",
    "        grn = False\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.do_ds = deep_supervision\n",
    "        assert checkpoint_style in [None, 'outside_block']\n",
    "        self.inside_block_checkpointing = False\n",
    "        self.outside_block_checkpointing = False\n",
    "        assert dim in ['2d', '3d']\n",
    "        if kernel_size is not None:\n",
    "            enc_kernel_size = kernel_size\n",
    "            dec_kernel_size = kernel_size\n",
    "\n",
    "        if dim == '2d':\n",
    "            conv = nn.Conv2d\n",
    "        elif dim == '3d':\n",
    "            conv = nn.Conv3d\n",
    "            \n",
    "        self.stem = conv(in_channels, n_channels, kernel_size=1)\n",
    "        if type(exp_r) == int:\n",
    "            exp_r = [exp_r for i in range(len(block_counts))]\n",
    "        \n",
    "        self.enc_block_0 = nn.Sequential(*[\n",
    "            MedNeXtBlock(\n",
    "                in_channels=n_channels,\n",
    "                out_channels=n_channels,\n",
    "                exp_r=exp_r[0],\n",
    "                kernel_size=enc_kernel_size,\n",
    "                do_res=do_res,\n",
    "                norm_type=norm_type,\n",
    "                dim=dim,\n",
    "                grn=grn\n",
    "                ) \n",
    "            for i in range(block_counts[0])]\n",
    "        ) \n",
    "\n",
    "        self.down_0 = MedNeXtDownBlock(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=2*n_channels,\n",
    "            exp_r=exp_r[1],\n",
    "            kernel_size=enc_kernel_size,\n",
    "            do_res=do_res_up_down,\n",
    "            norm_type=norm_type,\n",
    "            dim=dim\n",
    "        )\n",
    "    \n",
    "        self.enc_block_1 = nn.Sequential(*[\n",
    "            MedNeXtBlock(\n",
    "                in_channels=n_channels*2,\n",
    "                out_channels=n_channels*2,\n",
    "                exp_r=exp_r[1],\n",
    "                kernel_size=enc_kernel_size,\n",
    "                do_res=do_res,\n",
    "                norm_type=norm_type,\n",
    "                dim=dim,\n",
    "                grn=grn\n",
    "                )\n",
    "            for i in range(block_counts[1])]\n",
    "        )\n",
    "\n",
    "        self.down_1 = MedNeXtDownBlock(\n",
    "            in_channels=2*n_channels,\n",
    "            out_channels=4*n_channels,\n",
    "            exp_r=exp_r[2],\n",
    "            kernel_size=enc_kernel_size,\n",
    "            do_res=do_res_up_down,\n",
    "            norm_type=norm_type,\n",
    "            dim=dim,\n",
    "            grn=grn\n",
    "        )\n",
    "\n",
    "        self.enc_block_2 = nn.Sequential(*[\n",
    "            MedNeXtBlock(\n",
    "                in_channels=n_channels*4,\n",
    "                out_channels=n_channels*4,\n",
    "                exp_r=exp_r[2],\n",
    "                kernel_size=enc_kernel_size,\n",
    "                do_res=do_res,\n",
    "                norm_type=norm_type,\n",
    "                dim=dim,\n",
    "                grn=grn\n",
    "                )\n",
    "            for i in range(block_counts[2])]\n",
    "        )\n",
    "\n",
    "        self.down_2 = MedNeXtDownBlock(\n",
    "            in_channels=4*n_channels,\n",
    "            out_channels=8*n_channels,\n",
    "            exp_r=exp_r[3],\n",
    "            kernel_size=enc_kernel_size,\n",
    "            do_res=do_res_up_down,\n",
    "            norm_type=norm_type,\n",
    "            dim=dim,\n",
    "            grn=grn\n",
    "        )\n",
    "        \n",
    "        self.enc_block_3 = nn.Sequential(*[\n",
    "            MedNeXtBlock(\n",
    "                in_channels=n_channels*8,\n",
    "                out_channels=n_channels*8,\n",
    "                exp_r=exp_r[3],\n",
    "                kernel_size=enc_kernel_size,\n",
    "                do_res=do_res,\n",
    "                norm_type=norm_type,\n",
    "                dim=dim,\n",
    "                grn=grn\n",
    "                )            \n",
    "            for i in range(block_counts[3])]\n",
    "        )\n",
    "        \n",
    "        self.down_3 = MedNeXtDownBlock(\n",
    "            in_channels=8*n_channels,\n",
    "            out_channels=16*n_channels,\n",
    "            exp_r=exp_r[4],\n",
    "            kernel_size=enc_kernel_size,\n",
    "            do_res=do_res_up_down,\n",
    "            norm_type=norm_type,\n",
    "            dim=dim,\n",
    "            grn=grn\n",
    "        )\n",
    "\n",
    "        self.bottleneck = nn.Sequential(*[\n",
    "            MedNeXtBlock(\n",
    "                in_channels=n_channels*16,\n",
    "                out_channels=n_channels*16,\n",
    "                exp_r=exp_r[4],\n",
    "                kernel_size=dec_kernel_size,\n",
    "                do_res=do_res,\n",
    "                norm_type=norm_type,\n",
    "                dim=dim,\n",
    "                grn=grn\n",
    "                )\n",
    "            for i in range(block_counts[4])]\n",
    "        )\n",
    "\n",
    "        self.up_3 = MedNeXtUpBlock(\n",
    "            in_channels=16*n_channels,\n",
    "            out_channels=8*n_channels,\n",
    "            exp_r=exp_r[5],\n",
    "            kernel_size=dec_kernel_size,\n",
    "            do_res=do_res_up_down,\n",
    "            norm_type=norm_type,\n",
    "            dim=dim,\n",
    "            grn=grn\n",
    "        )\n",
    "\n",
    "        self.dec_block_3 = nn.Sequential(*[\n",
    "            MedNeXtBlock(\n",
    "                in_channels=n_channels*8,\n",
    "                out_channels=n_channels*8,\n",
    "                exp_r=exp_r[5],\n",
    "                kernel_size=dec_kernel_size,\n",
    "                do_res=do_res,\n",
    "                norm_type=norm_type,\n",
    "                dim=dim,\n",
    "                grn=grn\n",
    "                )\n",
    "            for i in range(block_counts[5])]\n",
    "        )\n",
    "\n",
    "        self.up_2 = MedNeXtUpBlock(\n",
    "            in_channels=8*n_channels,\n",
    "            out_channels=4*n_channels,\n",
    "            exp_r=exp_r[6],\n",
    "            kernel_size=dec_kernel_size,\n",
    "            do_res=do_res_up_down,\n",
    "            norm_type=norm_type,\n",
    "            dim=dim,\n",
    "            grn=grn\n",
    "        )\n",
    "\n",
    "        self.dec_block_2 = nn.Sequential(*[\n",
    "            MedNeXtBlock(\n",
    "                in_channels=n_channels*4,\n",
    "                out_channels=n_channels*4,\n",
    "                exp_r=exp_r[6],\n",
    "                kernel_size=dec_kernel_size,\n",
    "                do_res=do_res,\n",
    "                norm_type=norm_type,\n",
    "                dim=dim,\n",
    "                grn=grn\n",
    "                )\n",
    "            for i in range(block_counts[6])]\n",
    "        )\n",
    "\n",
    "        self.up_1 = MedNeXtUpBlock(\n",
    "            in_channels=4*n_channels,\n",
    "            out_channels=2*n_channels,\n",
    "            exp_r=exp_r[7],\n",
    "            kernel_size=dec_kernel_size,\n",
    "            do_res=do_res_up_down,\n",
    "            norm_type=norm_type,\n",
    "            dim=dim,\n",
    "            grn=grn\n",
    "        )\n",
    "\n",
    "        self.dec_block_1 = nn.Sequential(*[\n",
    "            MedNeXtBlock(\n",
    "                in_channels=n_channels*2,\n",
    "                out_channels=n_channels*2,\n",
    "                exp_r=exp_r[7],\n",
    "                kernel_size=dec_kernel_size,\n",
    "                do_res=do_res,\n",
    "                norm_type=norm_type,\n",
    "                dim=dim,\n",
    "                grn=grn\n",
    "                )\n",
    "            for i in range(block_counts[7])]\n",
    "        )\n",
    "\n",
    "        self.up_0 = MedNeXtUpBlock(\n",
    "            in_channels=2*n_channels,\n",
    "            out_channels=n_channels,\n",
    "            exp_r=exp_r[8],\n",
    "            kernel_size=dec_kernel_size,\n",
    "            do_res=do_res_up_down,\n",
    "            norm_type=norm_type,\n",
    "            dim=dim,\n",
    "            grn=grn\n",
    "        )\n",
    "\n",
    "        self.dec_block_0 = nn.Sequential(*[\n",
    "            MedNeXtBlock(\n",
    "                in_channels=n_channels,\n",
    "                out_channels=n_channels,\n",
    "                exp_r=exp_r[8],\n",
    "                kernel_size=dec_kernel_size,\n",
    "                do_res=do_res,\n",
    "                norm_type=norm_type,\n",
    "                dim=dim,\n",
    "                grn=grn\n",
    "                )\n",
    "            for i in range(block_counts[8])]\n",
    "        )\n",
    "\n",
    "        self.out_0 = OutBlock(in_channels=n_channels, n_classes=n_classes, dim=dim)\n",
    "\n",
    "        # Used to fix PyTorch checkpointing bug\n",
    "        self.dummy_tensor = nn.Parameter(torch.tensor([1.]), requires_grad=True)  \n",
    "\n",
    "        if deep_supervision:\n",
    "            self.out_1 = OutBlock(in_channels=n_channels*2, n_classes=n_classes, dim=dim)\n",
    "            self.out_2 = OutBlock(in_channels=n_channels*4, n_classes=n_classes, dim=dim)\n",
    "            self.out_3 = OutBlock(in_channels=n_channels*8, n_classes=n_classes, dim=dim)\n",
    "            self.out_4 = OutBlock(in_channels=n_channels*16, n_classes=n_classes, dim=dim)\n",
    "\n",
    "        self.block_counts = block_counts\n",
    "\n",
    "\n",
    "    def iterative_checkpoint(self, sequential_block, x):\n",
    "        \"\"\"\n",
    "        This simply forwards x through each block of the sequential_block while\n",
    "        using gradient_checkpointing. This implementation is designed to bypass\n",
    "        the following issue in PyTorch's gradient checkpointing:\n",
    "        https://discuss.pytorch.org/t/checkpoint-with-no-grad-requiring-inputs-problem/19117/9\n",
    "        \"\"\"\n",
    "        for l in sequential_block:\n",
    "            x = checkpoint.checkpoint(l, x, self.dummy_tensor)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.stem(x)\n",
    "        \n",
    "        x_res_0 = self.enc_block_0(x)\n",
    "        x = self.down_0(x_res_0)\n",
    "        x_res_1 = self.enc_block_1(x)\n",
    "        x = self.down_1(x_res_1)\n",
    "        x_res_2 = self.enc_block_2(x)\n",
    "        x = self.down_2(x_res_2)\n",
    "        x_res_3 = self.enc_block_3(x)\n",
    "        x = self.down_3(x_res_3)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        if self.do_ds:\n",
    "            x_ds_4 = self.out_4(x)\n",
    "\n",
    "        x_up_3 = self.up_3(x)\n",
    "        dec_x = x_res_3 + x_up_3 \n",
    "        x = self.dec_block_3(dec_x)\n",
    "\n",
    "        if self.do_ds:\n",
    "            x_ds_3 = self.out_3(x)\n",
    "        del x_res_3, x_up_3\n",
    "\n",
    "        x_up_2 = self.up_2(x)\n",
    "        dec_x = x_res_2 + x_up_2 \n",
    "        x = self.dec_block_2(dec_x)\n",
    "        if self.do_ds:\n",
    "            x_ds_2 = self.out_2(x)\n",
    "        del x_res_2, x_up_2\n",
    "\n",
    "        x_up_1 = self.up_1(x)\n",
    "        dec_x = x_res_1 + x_up_1 \n",
    "        x = self.dec_block_1(dec_x)\n",
    "        if self.do_ds:\n",
    "            x_ds_1 = self.out_1(x)\n",
    "        del x_res_1, x_up_1\n",
    "\n",
    "        x_up_0 = self.up_0(x)\n",
    "        dec_x = x_res_0 + x_up_0 \n",
    "        x = self.dec_block_0(dec_x)\n",
    "        del x_res_0, x_up_0, dec_x\n",
    "\n",
    "        x = self.out_0(x)\n",
    "\n",
    "        if self.do_ds:\n",
    "            return [x, x_ds_1, x_ds_2, x_ds_3, x_ds_4]\n",
    "        else: \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f740fcc-c9cb-4601-9ff2-8cf6140001fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mednextv1_small(num_input_channels, num_classes, kernel_size=3, ds=False):\n",
    "\n",
    "    return MedNeXt(\n",
    "        in_channels = num_input_channels, \n",
    "        n_channels = 32,\n",
    "        n_classes = num_classes, \n",
    "        exp_r=2,                         \n",
    "        kernel_size=kernel_size,         \n",
    "        deep_supervision=ds,             \n",
    "        do_res=True,                     \n",
    "        do_res_up_down = True,\n",
    "        block_counts = [2,2,2,2,2,2,2,2,2]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_mednextv1_base(num_input_channels, num_classes, kernel_size=3, ds=False):\n",
    "\n",
    "    return MedNeXt(\n",
    "        in_channels = num_input_channels, \n",
    "        n_channels = 32,\n",
    "        n_classes = num_classes, \n",
    "        exp_r=[2,3,4,4,4,4,4,3,2],       \n",
    "        kernel_size=kernel_size,         \n",
    "        deep_supervision=ds,             \n",
    "        do_res=True,                     \n",
    "        do_res_up_down = True,\n",
    "        block_counts = [2,2,2,2,2,2,2,2,2]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_mednextv1_medium(num_input_channels, num_classes, kernel_size=3, ds=False):\n",
    "\n",
    "    return MedNeXt(\n",
    "        in_channels = num_input_channels, \n",
    "        n_channels = 32,\n",
    "        n_classes = num_classes, \n",
    "        exp_r=[2,3,4,4,4,4,4,3,2],       \n",
    "        kernel_size=kernel_size,         \n",
    "        deep_supervision=ds,             \n",
    "        do_res=True,                     \n",
    "        do_res_up_down = True,\n",
    "        block_counts = [3,4,4,4,4,4,4,4,3],\n",
    "        checkpoint_style = 'outside_block'\n",
    "    )\n",
    "\n",
    "def create_mednextv1_large(num_input_channels, num_classes, kernel_size=3, ds=False):\n",
    "\n",
    "    return MedNeXt(\n",
    "        in_channels = num_input_channels, \n",
    "        n_channels = 32,\n",
    "        n_classes = num_classes, \n",
    "        exp_r=[3,4,8,8,8,8,8,4,3],                          \n",
    "        kernel_size=kernel_size,                     \n",
    "        deep_supervision=ds,             \n",
    "        do_res=True,                     \n",
    "        do_res_up_down = True,\n",
    "        block_counts = [3,4,8,8,8,8,8,4,3],\n",
    "        checkpoint_style = 'outside_block'\n",
    "    )\n",
    "\n",
    "def create_mednext_v1(num_input_channels, num_classes, model_id, kernel_size=3,\n",
    "                      deep_supervision=False):\n",
    "\n",
    "    model_dict = {\n",
    "        'S': create_mednextv1_small,\n",
    "        'B': create_mednextv1_base,\n",
    "        'M': create_mednextv1_medium,\n",
    "        'L': create_mednextv1_large,\n",
    "        }\n",
    "    \n",
    "    return model_dict[model_id](\n",
    "        num_input_channels, num_classes, kernel_size, deep_supervision\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2d9e4-0d14-4c86-94f5-d31e61e1f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n",
    "    y_true = y_true.to(torch.float32)\n",
    "    y_pred = (y_pred>thr).to(torch.float32)\n",
    "    inter = (y_true*y_pred).sum(dim=dim)\n",
    "    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n",
    "    dice = ((2*inter+epsilon)/(den+epsilon)).mean(dim=(1,0))\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c86895-df1b-44ca-8163-4fb4d619bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medpy import metric\n",
    "def calculate_metric_percase(gt, pred, thr=0.5):\n",
    "    gt = gt.cpu().detach().numpy()\n",
    "    pred = (pred>thr).cpu().detach().numpy()\n",
    "    dice = metric.binary.dc(pred, gt)\n",
    "    jc = metric.binary.jc(pred, gt)\n",
    "    hd = metric.binary.hd95(pred, gt)\n",
    "    asd = metric.binary.asd(pred, gt)\n",
    "    return dice, jc, hd, asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf5f00-ec62-477f-9d7d-359a2f7c9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(images, masks, preds, epoch, step):\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(9, 9))\n",
    "    for row in range(3):\n",
    "        ax[row, 0].imshow(images[row][1].cpu().squeeze(), cmap='gray')\n",
    "        ax[row, 0].set_title(f\"Epoch {epoch} Batch {step} Sample {row} - Image\")\n",
    "        ax[row, 1].imshow(masks[row].cpu().squeeze(), cmap='gray')\n",
    "        ax[row, 1].set_title(f\"Epoch {epoch} Batch {step} Sample {row} - Ground Truth\")\n",
    "        ax[row, 2].imshow(preds[row].cpu().squeeze(), cmap='gray')\n",
    "        ax[row, 2].set_title(f\"Epoch {epoch} Batch {step} Sample {row} - Prediction\")\n",
    "        for col in range(3):\n",
    "            ax[row, col].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5bb2c5-943b-42e5-a263-e2363be30537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SegLossFunc import SegLoss\n",
    "BoundaryDoULoss = SegLoss.BoundaryDoULoss()\n",
    "\n",
    "SoftDiceCLDiceBoundaryDoULoss = SegLoss.SoftDiceCLDiceBoundaryDoULoss()\n",
    "\n",
    "StructureLoss = SegLoss.StructureLoss()\n",
    "StructureLossBoundaryDOU = SegLoss.StructureLossBoundaryDOU()\n",
    "StructureLossBoundaryDOUV2 = SegLoss.StructureLossBoundaryDOUV2()\n",
    "\n",
    "JaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\n",
    "DiceLoss    = smp.losses.DiceLoss(mode='multilabel')\n",
    "BCELoss     = smp.losses.SoftBCEWithLogitsLoss()\n",
    "LovaszLoss  = smp.losses.LovaszLoss(mode='multilabel', per_image=False)\n",
    "TverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n",
    "FocalLoss = smp.losses.FocalLoss(mode=\"multilabel\")\n",
    "\n",
    "def criterion(y_pred, y_true):\n",
    "    return BoundaryDoULoss(y_pred, y_true) + 0.5*BCELoss(y_pred, y_true) + 0.5*DiceLoss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df8bf0-4187-4c89-9975-1ed3f931b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义逐渐升温调度器\n",
    "class GradualWarmupSchedulerV3(GradualWarmupScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV3, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch >= self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8650f22-fb3e-4c7b-8307-f11a7a1453ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    scaler = GradScaler()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    for step, (images, bi_masks, multi_mask) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        if CFG.num_classes == 1:\n",
    "            masks = bi_masks.to(device, dtype=torch.float)\n",
    "        else:\n",
    "            masks = multi_mask.to(device, dtype=torch.float)\n",
    "        batch_size = images.size(0)\n",
    "        with autocast(enabled=True):\n",
    "            y_preds = model(images)\n",
    "            loss = criterion(y_preds, masks)\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.accum_iter > 1:\n",
    "            loss = loss / CFG.accum_iter\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.accum_iter == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            cusprint('Epoch: [{0}][{1}/{2}] '\n",
    "                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                'Grad: {grad_norm:.4f}  '\n",
    "                'LR: {lr:.7f}  '\n",
    "                .format(\n",
    "                epoch, step, len(train_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses,\n",
    "                remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                grad_norm=grad_norm,\n",
    "                lr=optimizer.param_groups[0][\"lr\"],\n",
    "                ))\n",
    "    return losses.avg, optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f3401-d12a-427e-aa1c-c86d6da89dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch(valid_loader, model, criterion, device,epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    start = end = time.time()\n",
    "    val_scores = []\n",
    "    for step, (images, bi_masks, multi_mask) in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        if CFG.num_classes == 1:\n",
    "            masks = bi_masks.to(device, dtype=torch.float)\n",
    "        else:\n",
    "            masks = multi_mask.to(device, dtype=torch.float)\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(images)\n",
    "\n",
    "        loss = criterion(y_pred, masks)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        # record accuracy\n",
    "        y_pred = y_pred.sigmoid() ####\n",
    "        # y_pred = y_pred.sigmoid().to('cpu').numpy()\n",
    "        \n",
    "        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n",
    "        #val_scores.append([val_dice])\n",
    "        dice, jc, hd, asd = calculate_metric_percase(masks, y_pred)\n",
    "\n",
    "        val_scores.append([val_dice, dice, jc, hd, asd])\n",
    "        \n",
    "        if CFG.accum_iter > 1:\n",
    "            loss = loss / CFG.accum_iter\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            cusprint('EVAL: [{0}/{1}] '\n",
    "                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                .format(\n",
    "                step, len(valid_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses,\n",
    "                remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                ))\n",
    "    val_scores = np.mean(val_scores, axis=0)\n",
    "    \n",
    "    if val_scores[0]>0.95:\n",
    "        selected_indices = np.random.choice(images.shape[0], 3, replace=False)\n",
    "        images = images[selected_indices]\n",
    "        masks = masks[selected_indices]\n",
    "        preds = y_pred[selected_indices]\n",
    "        plot_examples(images, masks, preds,epoch,step)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return losses.avg, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523ffb2-a370-458c-bf4f-33ef60c72637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train_loop(train_df, fold_1, fold_2, criterion):\n",
    "    loginfo(f\"========== training ==========\")\n",
    "    # ====================================================\n",
    "    # loader \n",
    "    # ====================================================\n",
    "    train_folds = train_df[train_df[\"fold\"].isin(fold_1)].reset_index(drop=True)\n",
    "    valid_folds = train_df[train_df[\"fold\"].isin(fold_2)].reset_index(drop=True)\n",
    "\n",
    "    train_img,train_label=load_data(train_folds)\n",
    "    valid_img,valid_label=load_data(valid_folds)\n",
    "    \n",
    "    train_dataset = Spine_Dataset([train_img],[train_label],arg=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.train_batch_size ,num_workers=0, shuffle=True, pin_memory=True)\n",
    "    valid_dataset = Spine_Dataset([valid_img],[valid_label],arg=False)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_batch_size ,num_workers=0, shuffle=False, pin_memory=True)\n",
    "    # ====================================================\n",
    "    # model & optimizer & scheduler & loss \n",
    "    # ====================================================\n",
    "    model = WIDRMS().cuda()\n",
    "    #model = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
    "    # optimizer\n",
    "    if CFG.optimizer == \"AdamW\":\n",
    "        if CFG.scheduler_warmup in [\"GradualWarmupSchedulerV2\",\"GradualWarmupSchedulerV3\"]:\n",
    "            optimizer = AdamW(model.parameters(), lr=CFG.lr/CFG.warmup_factor, weight_decay=CFG.weight_decay) \n",
    "        else:\n",
    "            optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)  \n",
    "    # scheduler\n",
    "    if CFG.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "    elif CFG.scheduler=='CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "    elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "\n",
    "    if CFG.scheduler_warmup==\"GradualWarmupSchedulerV3\":\n",
    "        scheduler_warmup = GradualWarmupSchedulerV3(optimizer, multiplier=10, total_epoch=CFG.warmup_epo, after_scheduler=scheduler)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop \n",
    "    # ====================================================\n",
    "\n",
    "    valid_acc_max=0\n",
    "    valid_acc_max_cnt=0\n",
    "    for epoch in range(CFG.epochs):\n",
    "        loginfo(f\"***** Epoch {epoch} *****\")\n",
    "        if CFG.scheduler_warmup in [\"GradualWarmupSchedulerV2\",\"GradualWarmupSchedulerV3\"]:\n",
    "            pass\n",
    "            # loginfo(f\"schwarmup_last_epoch:{scheduler_warmup.last_epoch}, schwarmup_lr:{scheduler_warmup.get_last_lr()[0]}\")\n",
    "        if CFG.scheduler=='CosineAnnealingLR':\n",
    "            loginfo(f\"scheduler_last_epoch:{scheduler.last_epoch}, scheduler_lr:{scheduler.get_last_lr()[0]}\")\n",
    "        loginfo(f\"optimizer_lr:{optimizer.param_groups[0]['lr']}\")\n",
    "                \n",
    "        start_time = time.time() # 记录当前时间\n",
    "        avg_loss, cur_lr = train_one_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "        avg_val_loss, valid_scores = valid_one_epoch(valid_loader, model, criterion, device,epoch)\n",
    "        # scoring\n",
    "        elapsed = time.time() - start_time\n",
    "        # print(\"valid_scores:\", valid_scores, type(valid_scores))\n",
    "        val_dice, dice, jc, hd, asd = valid_scores\n",
    "        \n",
    "        loginfo(f'Epoch {epoch} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        loginfo(f'Epoch {epoch} -val_dice: {val_dice:.4f} - Dice Score: {dice:.4f}- Jaccard Score: {jc:.4f}- HD95 Score: {hd:.4f}- ASSD Score: {asd:.4f}')\n",
    "    \n",
    "        if CFG.scheduler_warmup in [\"GradualWarmupSchedulerV2\",\"GradualWarmupSchedulerV3\"]:\n",
    "            scheduler_warmup.step()\n",
    "        elif CFG.scheduler == \"ReduceLROnPlateau\":\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif CFG.scheduler in [\"CosineAnnealingLR\", \"CosineAnnealingWarmRestarts\"]:\n",
    "            scheduler.step()\n",
    "\n",
    "        torch.save({'model': model.state_dict()}, CFG.output_dir+f'/{CFG.model_name}_{CFG.exp_name}_fold{fold}_epoch{epoch}.pth')\n",
    "        \n",
    "        # early stopping \n",
    "        if val_dice > valid_acc_max:\n",
    "            valid_acc_max = dice\n",
    "            valid_acc_max_cnt=0\n",
    "            best_acc_epoch = epoch\n",
    "        else:\n",
    "            valid_acc_max_cnt+=1\n",
    "        \n",
    "        if valid_acc_max_cnt >= CFG.n_early_stopping:\n",
    "            torch.save({'model': model.state_dict()}, CFG.output_dir+f'/{CFG.model_name}_{CFG.exp_name}_fold{fold}_epoch{epoch}.pth')\n",
    "        \n",
    "            print(\"early_stopping\")\n",
    "            break\n",
    "        \n",
    "        torch.save({'model': model.state_dict()}, CFG.output_dir+f'/{CFG.model_name}_{CFG.exp_name}_fold{fold}_epoch{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d6a0f-b36b-4f8b-8ab0-da9b92226997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid():\n",
    "    fold_1 = CFG.train_fold_list\n",
    "    fold_2 = CFG.valid_fold_list\n",
    "    train_loop(df, fold_1, fold_2, criterion)\n",
    "def test():\n",
    "    fold_1 = CFG.train_fold_list\n",
    "    fold_2 = CFG.test_fold_list\n",
    "    train_loop(df, fold_1, fold_2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157696e-ffcc-40cd-b6f4-92aaa4a89a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
